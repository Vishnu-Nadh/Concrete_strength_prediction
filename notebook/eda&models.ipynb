{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../Training_Batch_Files/Concrete_Data.csv', names = ['cement', 'blast_furnace_slag', 'fly_ash', 'water', 'super_plasticizer','coarse_aggregate', 'fine_aggregate', 'age', 'compressive_strength'], skiprows=[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()\n",
    "data.isin([np.inf, -np.inf]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtale\n",
    "sheet = dtale.show(data)\n",
    "sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from the exploratory data analysis it can be found that \n",
    "- cement, water, coarse aggragate, fine aggragate have aproximate normal distribution\n",
    "- Rest of the features are left skewed and can be normalised by log transformation\n",
    "- Some of the features show somewhat linear relation with the target variable hence linear regression might be a good choice\n",
    "- No high correlations are shown between the independent features hence all features can be considered for model building\n",
    "- Outliers are relatevely very few and hence need not be concerned.\n",
    "- scaling is a must becouse meny features and huge difference in its magnitudes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "y = data.compressive_strength\n",
    "x_tr = data.drop('compressive_strength', axis=1).copy()\n",
    "x_tr = x_tr.apply(lambda x:np.log1p(x), axis=1) # log transformation\n",
    "# def boxcox_tr(data): # box cox transformation\n",
    "#     columns = data.columns\n",
    "#     for col in columns:\n",
    "#         arr_, lmda = stats.boxcox(data[col] + 0.001)\n",
    "#         data[col] = arr_\n",
    "#     return data\n",
    "\n",
    "# data_tr_bx = boxcox_tr(data_tr)\n",
    "for col in  x_tr.columns:\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    sns.distplot(data[col], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"original data\", color =\"green\", ax = ax[0])\n",
    "    sns.distplot(x_tr[col], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"log normalised\", color =\"blue\", ax = ax[1])\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(10)\n",
    "    plt.title(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x_tr), columns=x_tr.columns)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature transformation and preprocessing completed. Lets try following algorithms\n",
    "1. Linear regression (degree 1), ridge, lasso, \n",
    "2. Linear regression (degree 2), ridge, lasso, \n",
    "3. Linear regression (degree 3), ridge, lasso,\n",
    "4. Random Forest Regressor\n",
    "5. Xgboost\n",
    "6. Adaboost\n",
    "7. SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "def fitAndPrintScore(model, model_name, x_train, y_train, x_test, y_test, deg=None):\n",
    "    model.fit(x_train, y_train)\n",
    "    model_train_score = model.score(x_train, y_train)\n",
    "    model_test_score = model.score(x_test, y_test)\n",
    "    if deg != None:\n",
    "        print(f\"for {model_name} (degree={deg}) train score :{model_train_score}, test score:{model_test_score}\")\n",
    "    else:\n",
    "        print(f\"for {model_name} train score :{model_train_score}, test score:{model_test_score}\")\n",
    "\n",
    "poly_degs = [1, 2, 3]\n",
    "models = {'Linear Regression':LinearRegression(), 'Ridge regression':Ridge(), 'Lasso Regression':Lasso(), 'Elastic net Regression':ElasticNet()}\n",
    "for deg in poly_degs:\n",
    "    poly = PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    x_p = poly.fit_transform(x)\n",
    "    x_p_train, x_p_test, y_p_train, y_p_test = train_test_split(x_p, y, test_size=0.2, random_state=100)\n",
    "    for model_name, model in models.items():\n",
    "        fitAndPrintScore(model, model_name, x_p_train, y_p_train, x_p_test, y_p_test, deg = deg)\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### considering the overfitting and test score polynomial ridge regression with degree 2 will be a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "x_p = poly.fit_transform(x)\n",
    "y_p = y\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'alpha':trial.suggest_float('alpha', 1e-10, 10, log = True),\n",
    "        # 'fit_intercept':trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'solver': trial.suggest_categorical('solver', ['sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag' , 'saga'])\n",
    "    }\n",
    "    \n",
    "    x_p_train, x_p_test, y_p_train, y_p_test = train_test_split(x_p, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    ridge = Ridge(**params)\n",
    "    ridge.fit(x_p_train, y_p_train)\n",
    "    test_score = ridge.score(x_p_test, y_p_test)\n",
    "    return test_score    \n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials = 200)\n",
    "# Ridge._get_param_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.params\n",
    "params\n",
    "# params = {'alpha': 9.99081794015465, 'solver': 'svd'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': 10, 'solver': 'svd'}\n",
    "x_p_train, x_p_test, y_p_train, y_p_test = train_test_split(x_p, y_p, test_size=0.2, random_state=100)\n",
    "\n",
    "ridge_reg = Ridge(**params)\n",
    "fitAndPrintScore(ridge_reg, \"Ridge regression\", x_p_train, y_p_train, x_p_test, y_p_test, deg=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "fitAndPrintScore(rf, 'Random Forest regression', x_train, y_train, x_test, y_test)\n",
    "\n",
    "rf._get_param_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# dt = DecisionTreeRegressor()\n",
    "# path = dt.cost_complexity_pruning_path(x_train, y_train)\n",
    "# ccp_alphas = path.ccp_alphas\n",
    "# ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning of random forest\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10,200),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 16)\n",
    "    }\n",
    "    rf_reg = RandomForestRegressor(**params)\n",
    "    rf_reg.fit(x_train, y_train)\n",
    "    test_score = rf_reg.score(x_test, y_test)\n",
    "    # train_score = rf_reg.score(x_train, y_train)\n",
    "    # if test_score >= 0.90 and train_score > test_score:\n",
    "    #     diff = train_score - test_score\n",
    "    # else:\n",
    "    #     diff = 1\n",
    "    # return diff\n",
    "\n",
    "    return test_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_params = {'bootstrap': False,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 24,\n",
    " 'n_estimators': 111,\n",
    " 'min_samples_split': 3}\n",
    "\n",
    "rf = RandomForestRegressor(**best_test_params)\n",
    "fitAndPrintScore(rf, 'RandomForestRegressor', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST \n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "fitAndPrintScore(rf, 'RandomForestRegressor', x_train, y_train, x_test, y_test)\n",
    "xgb.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning of xgbregressor\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'eta':trial.suggest_float('eta', 0.005, 0.5, log=True),\n",
    "        'gamma':trial.suggest_float('gamma', 0.01, 20, log = True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 16, log=True),\n",
    "        'subsample':trial.suggest_float('subsample', 0.5, 1),\n",
    "        'lambda':trial.suggest_float('lambda', 0.01, 10, log = True)\n",
    "    }\n",
    "    rf_reg = XGBRegressor(**params)\n",
    "    rf_reg.fit(x_train, y_train)\n",
    "    test_score = rf_reg.score(x_test, y_test)\n",
    "    # train_score = rf_reg.score(x_train, y_train)\n",
    "    # if test_score >= 0.90 and train_score > test_score:\n",
    "    #     diff = train_score - test_score\n",
    "    # else:\n",
    "    #     diff = 1\n",
    "    # return diff\n",
    "    return test_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'booster': 'dart',\n",
    " 'eta': 0.4632229410828487,\n",
    " 'gamma': 7.077192780534438,\n",
    " 'max_depth': 3,\n",
    " 'subsample': 0.8092309111849526,\n",
    " 'lambda': 0.015342068089733317}\n",
    "\n",
    "xgb_r = XGBRegressor(**params)\n",
    "\n",
    "fitAndPrintScore(xgb_r, 'xgboost regression', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ab_reg = AdaBoostRegressor()\n",
    "fitAndPrintScore(ab_reg, 'adaboost regression', x_train, y_train, x_test, y_test)\n",
    "# ab_reg._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning adaboost\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators':trial.suggest_int('n_estimators', 10, 500, step = 5),\n",
    "        'loss':trial.suggest_categorical('loss', ['linear', 'square', 'exponential']),\n",
    "        'learning_rate':trial.suggest_float('learning_rate', 10-5, 10, log=True)\n",
    "    }\n",
    "    adb = AdaBoostRegressor(**params)\n",
    "    adb.fit(x_train, y_train)\n",
    "    test_score = adb.score(x_test, y_test)\n",
    "    return test_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.params\n",
    "adaboost = AdaBoostRegressor(**params)\n",
    "fitAndPrintScore(adaboost, 'adaboost regression', x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "fitAndPrintScore(svr, \"Support vector machine reggression\", x_train, y_train, x_test, y_test)\n",
    "svr._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tuning svr\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'kernel':trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma':trial.suggest_float('gamma', 0.001, 1),\n",
    "        'epsilon':trial.suggest_float('epsilon', 0.001, 1)\n",
    "    }\n",
    "    adb = SVR(**params)\n",
    "    adb.fit(x_train, y_train)\n",
    "    test_score = adb.score(x_test, y_test)\n",
    "    return test_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_trial.params\n",
    "svr = SVR(**params)\n",
    "fitAndPrintScore(svr, \"Support vector machine reggression\", x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from the above expirimentations it can be decided to go with ridge reggression with degree 2, random_forest and xgboost for the model building with parameter tuning in the project. for clustering kmean clustering can be adopted before choosing each algorithm according to their performance on each cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def fitAndPrintScore(model, model_name, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    model_train_score = model.score(x_train, y_train)\n",
    "    model_test_score = model.score(x_test, y_test)\n",
    "    print(\n",
    "        f\"for {model_name} train score :{model_train_score}, test score:{model_test_score}\"\n",
    "    )\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../Training_Batch_Files/Concrete_Data.csv\")\n",
    "data.head()\n",
    "\n",
    "y = data.Concrete_compressive_strength\n",
    "x_tr = data.drop(\"Concrete_compressive_strength\", axis=1)\n",
    "\n",
    "x_tr = x_tr.apply(lambda x: np.log1p(x), axis=1)  # log transformation\n",
    "x_tr.head(10)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x_tr), columns=x_tr.columns)\n",
    "y\n",
    "\n",
    "x.head(5)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=100\n",
    ")\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "params = {\n",
    "    \"booster\": \"dart\",\n",
    "    \"eta\": 0.4632229410828487,\n",
    "    \"gamma\": 7.077192780534438,\n",
    "    \"max_depth\": 3,\n",
    "    \"subsample\": 0.8092309111849526,\n",
    "    \"lambda\": 0.015342068089733317,\n",
    "}\n",
    "\n",
    "xgb_r = XGBRegressor(**params)\n",
    "\n",
    "xgb_r.fit(x_train, y_train)\n",
    "model_train_score = xgb_r.score(x_train, y_train)\n",
    "model_test_score = xgb_r.score(x_test, y_test)\n",
    "print(f\"for xgb train score :{model_train_score}, test score:{model_test_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fitAndPrintScore(xgb_r, \"xgboost regression\", x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_r.predict(np.array([[1.781614,-1.059324,-0.900772,-0.897885, 0.662010,-2.374778,0.391357,0.112702]])))                   \n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_r.predict(x_test)[0:2]\n",
    "y_actual = y_test[0:2]\n",
    "print(y_pred)\n",
    "print(y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7fac7c0a35fa2f8729d3f7a5ec2fca10e84bd99ad2ea8e02dfba2b5c1da09e0f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('EDA_ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
